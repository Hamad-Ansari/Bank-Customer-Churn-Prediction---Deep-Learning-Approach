
# Bank Customer Churn Prediction - Deep Learning Approach

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange)
![License](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)

A comprehensive machine learning project for predicting bank customer churn using gradient boosting algorithms (LightGBM, XGBoost, CatBoost) and deep learning approaches.
![imresizer-1706204681767.jpg](attachment:imresizer-1706204681767.jpg)
## ğŸ“‹ Project Overview

This project analyzes customer data from a bank to predict whether customers will churn (leave the bank). The notebook demonstrates:
- **Exploratory Data Analysis (EDA)** of customer demographics and behavior
- **Feature Engineering** and preprocessing techniques
- **Multiple ML Model Training** with LightGBM, XGBoost, and CatBoost
- **Model Evaluation** and performance comparison
- **Churn Rate Analysis** (21.16% in the dataset)

## ğŸš€ Quick Start

### Prerequisites
```bash
Python 3.8+
Jupyter Notebook/Lab
```
# Model Performance Report

## Executive Summary
This report summarizes the performance of machine learning models trained to predict bank customer churn. The dataset contains 165,034 customer records with a churn rate of 21.16%.

## Models Evaluated
1. **LightGBM** - Gradient boosting framework
2. **XGBoost** - Optimized gradient boosting
3. **CatBoost** - Categorical feature handling

## Performance Metrics

### ROC-AUC Scores
| Model | ROC-AUC | Rank |
|-------|---------|------|
| CatBoost | 0.872 | 1 |
| LightGBM | 0.865 | 2 |
| XGBoost | 0.858 | 3 |

### Detailed Metrics

## ğŸ“Š Dataset
- The dataset contains customer information including:

- Demographics: Age, Gender, Geography

- Financial: CreditScore, Balance, EstimatedSalary

- Behavioral: Tenure, NumOfProducts, IsActiveMember

- Target: Exited (1 = Churned, 0 = Stayed)

## Dataset Statistics:

- Training samples: 165,034

- Test samples: 165,034

- Features: 14 (train), 13 (test)

- Churn rate: 21.16%

## ğŸ—ï¸ Project Structure
```
bank-churn-prediction/
â”‚
â”œâ”€â”€ bank_churn_dataset.ipynb      # Main Jupyter notebook
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ LICENSE                       # License file
â”œâ”€â”€ .gitignore                    # Git ignore file
â”œâ”€â”€ data/                         # Data directory
â”‚   â”œâ”€â”€ train.csv                 # Training dataset
â”‚   â””â”€â”€ test.csv                  # Test dataset
â”œâ”€â”€ models/                       # Saved models
â”‚   â”œâ”€â”€ lightgbm_model.pkl
â”‚   â”œâ”€â”€ xgboost_model.pkl
â”‚   â””â”€â”€ catboost_model.pkl
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ model_training.py
â”‚   â””â”€â”€ evaluation.py
â”œâ”€â”€ notebooks/                    # Additional notebooks
â”‚   â””â”€â”€ exploratory_analysis.ipynb
â”œâ”€â”€ reports/                      # Generated reports
â”‚   â””â”€â”€ model_performance.md
â”œâ”€â”€ config/                       # Configuration files
â”‚   â””â”€â”€ params.yaml
â””â”€â”€ tests/                        # Unit tests
    â””â”€â”€ test_preprocessing.py
```
 # ğŸ”§ Implementation Details
## Data Preprocessing
- Handling missing values

- Feature encoding (OneHotEncoder for categorical variables)

- Feature scaling (StandardScaler for numerical variables)

- Train-test split with stratification

## Models Implemented
- LightGBM - Gradient boosting framework by Microsoft

- XGBoost - Optimized gradient boosting library

- CatBoost - Handles categorical features natively

- Deep Learning Model (Planned for future work)

## Evaluation Metrics
- ROC-AUC Score (Primary metric)

- Accuracy, Precision, Recall

- Confusion Matrix Analysis

- Feature Importance Analysis

# ğŸ“ˆ Results
## Performance Summary   
l	ROC-AUC Score	Accuracy	Precision	Recall
LightGBM	0.86	0.85	0.78	0.63
XGBoost	0.85	0.84	0.76	0.61
CatBoost	0.87	0.86	0.79	0.65

## Key Findings
 - Age and Balance are the most important predictors of churn

- German customers have higher churn rates compared to French/Spanish

- Inactive members are more likely to churn

- Customers with 2 products show lowest churn rates
# ğŸ“± Usage
## Running the Complete Pipeline  
# ğŸ‘¨â€ğŸ’» Author
## Hammad Zahid

- LinkedIn: https//linkedin.com/in/hammad-zahid-xyz

- GitHub: https//github.com/Hamad-Ansari

- Email: mrhammadzahi24@gmail.com

## ğŸ™ Acknowledgments
- Dataset sourced from Kaggle Bank Customer Churn Prediction

- Thanks to all contributors and maintainers of the ML libraries used

- Inspired by real-world banking analytics problems

